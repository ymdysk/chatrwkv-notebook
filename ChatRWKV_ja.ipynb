{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymdysk/chatrwkv-notebook/blob/main/ChatRWKV_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoLtM9B2vKwm"
      },
      "source": [
        "# ChatRWKVをGoogle Colabで実行するnotebook\n",
        "## 動作概要\n",
        "- Google Driveをマウントし、srcフォルダにChatRWKVのGitリポジトリをcloneします。\n",
        "- ChatRWKV/v2/ フォルダに移動してモデルをダウンロードします。\n",
        "- モデルに関する変数はフォームで設定し、chat.pyの内容を一部置換してchat-notebook.pyに保存してモデルをロードします。\n",
        "- 文章生成に関する変数を設定してチャットを実行します。\n",
        "\n",
        "## Tips\n",
        "- GPUを使いたい場合はGoogle Colabの「ランタイム」メニュー→「ランタイムのタイプを変更」→「ハードウェアアクセラレータ」で「GPU」を選択して保存します。\n",
        "- GPUが使えない場合は strategy = 'cpu fp32', RWKV_CUDA_ON = 0 を設定すると動作しますが、生成速度は遅くなります。\n",
        "- 使用するモデルは、約3GBの1B(10億)パラメータのモデルか約6GBの3B(30億)パラメータのモデルをMODEL_URLで選択するか、任意のURLを入力します。パラメータ数のより多いモデルを使うとより賢い応答ができますが、ファイル容量も大きくなり、VRAMやRAM等の要求スペックも上がります。\n",
        "- Google Colaboratoryの無料版では1B,3B迄のモデルは実行できるようです。\n",
        "- Googleの無料ストレージは15GBですが、Google Driveにモデルを保存すると逼迫しやすいです。\n",
        "- 2023年4月21日時点では、Hugging Faceで公開されている各モデルの日本語の割合は0～10%程度です。日本語の応答をより自然にするためには、日本語の割合の高いデータでプレトレーニングしたり、ファインチューニングしたりする必要があると思われます。\n",
        "\n",
        "## License\n",
        "- このnotebook  \n",
        "  Copyright 2023 Yosuke Yamada  \n",
        "  Licensed under the Apache License, Version 2.0  \n",
        "  http://www.apache.org/licenses/LICENSE-2.0\n",
        "- notebookから使用/ダウンロードするソフトウェアやモデルのライセンスは個別に確認してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 環境の確認"
      ],
      "metadata": {
        "id": "DmPHwlbpGBGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvf3g-UOHp3D",
        "outputId": "cf8344fc-8143-46af-d42a-51de8b030137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 23 05:27:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# NVIDIAのシステム管理インターフェイスでCUDA環境の状況を確認(GPU用。CPUを使う場合は実行不要)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHuA67Z8I-sw",
        "outputId": "699527eb-9247-41aa-8e5a-a46faef4e3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "# Cuda compilerのバージョンを確認(GPU用。CPUを使う場合は実行不要)\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 環境設定"
      ],
      "metadata": {
        "id": "oYMQDqw4GRYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デフォルトのtorchでは高速化のためRWKV_CUDA_ON = 1を設定した場合に、CUDAカーネルのビルドに失敗することがあるので入れなおす(初回のみ)\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==2.0.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr70F9lc_r-l",
        "outputId": "2d73b3cb-b38f-4e3f-a1d8-8de6a150309c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.0+cu118\n",
            "Uninstalling torch-2.0.0+cu118:\n",
            "  Successfully uninstalled torch-2.0.0+cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.0.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp39-cp39-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m772.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0+cu118) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0+cu118) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0+cu118) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bb6gKeVT0Pe",
        "outputId": "00d06dc0-86cb-4b39-9110-a5773557263c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# PytorchでCUDAが使えるか確認する。使える場合、それぞれTrue, デバイス番号が返される。失敗する場合は上のセルを再実行する。CPUを使う場合(GPUを使わない場合)は実行不要。\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UYqFX-DvU4C",
        "outputId": "0952816d-c384-4b24-9271-22ec2695b993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/src\n"
          ]
        }
      ],
      "source": [
        "# Googleドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# ソースコード配置先フォルダがなければ作成してcdする\n",
        "import os\n",
        "os.makedirs(\"/content/drive/My Drive/src\", exist_ok=True)\n",
        "%cd '/content/drive/My Drive/src'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSMAkkVbG4K0",
        "outputId": "3590074a-6166-483a-b604-eaa66b597e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatRWKV'...\n",
            "remote: Enumerating objects: 1340, done.\u001b[K\n",
            "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 1340 (delta 88), reused 162 (delta 52), pack-reused 1124\u001b[K\n",
            "Receiving objects: 100% (1340/1340), 26.97 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (723/723), done.\n"
          ]
        }
      ],
      "source": [
        "# ChatRWKVのソースコードをgit cloneで取得(初回のみ)\n",
        "!git clone https://github.com/BlinkDL/ChatRWKV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KrxdGeUGJ7X",
        "outputId": "d692959a-fb24-4331-88b8-e75ab969810e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/src/ChatRWKV/v2\n"
          ]
        }
      ],
      "source": [
        "# ChatRWKV/v2フォルダに移動\n",
        "%cd 'ChatRWKV/v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG4WhkbLJdpy",
        "outputId": "eef8903a-07fd-42ee-98e8-5758e062529f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rwkv\n",
            "  Downloading rwkv-0.7.3-py3-none-any.whl (16 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, ninja, rwkv\n",
            "Successfully installed ninja-1.11.1 rwkv-0.7.3 tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "# rwkv, ninjaパッケージをpipでインストール\n",
        "!pip install rwkv ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの準備"
      ],
      "metadata": {
        "id": "bXvywPMzGlXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hTTbe9fB0Cn"
      },
      "outputs": [],
      "source": [
        "# モデルの設定・ダウンロード\n",
        "# Raven  https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main\n",
        "# その他 https://huggingface.co/BlinkDL\n",
        "MODEL_URL = 'https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main/RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096.pth' #@param ['https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main/RWKV-4-Raven-1B5-v10-Eng99%25-Other1%25-20230418-ctx4096.pth', 'https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main/RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096.pth\"]  {allow-input: true}\n",
        "!curl -OLC - $MODEL_URL\n",
        "# MODEL_NAME は MODEL_URL の最後の「/」記号以降の文字列(ファイル名)から、拡張子の「.pth」を除いたもの\n",
        "MODEL_NAME = MODEL_URL[MODEL_URL.rfind('/') + 1:].rstrip('.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm3QS13qlax2"
      },
      "outputs": [],
      "source": [
        "# RWKVモデルの動作に関する設定\n",
        "strategy = 'cuda fp16' #@param ['cpu fp32', 'cuda fp16', 'cuda:0 fp16 -> cuda:1 fp16', 'cuda fp16i8 *10 -> cuda fp16', 'cuda fp16i8', 'cuda fp16i8 -> cpu fp32 *10', 'cuda fp16i8 *10+'] {allow-input: true}\n",
        "# 'cpu fp32' # CUDAが使えずCPUを使う場合\n",
        "# 'cuda fp16' # CUDAが使える場合のデフォルト値\n",
        "# 'cuda:0 fp16 -> cuda:1 fp16' # 2枚のGPUが使える場合\n",
        "# 'cuda fp16i8 *10 -> cuda fp16' # 最初は10層cuda int8量子化、残りはcuda fp16i8\n",
        "# 'cuda fp16i8' # 全層cuda int8量子化\n",
        "# 'cuda fp16i8 -> cpu fp32 *10' # 最初はcuda fp16i8、後続の10層はcpu fp32\n",
        "# 'cuda fp16i8 *10+' # 最初は10層cuda int8量子化、残りは必要に応じて動的ロード\n",
        "\n",
        "# CUDAが使える場合は1、使えない場合は0にする\n",
        "RWKV_CUDA_ON = \"1\" #@param [0, 1]\n",
        "\n",
        "# チャットで使う言語\n",
        "CHAT_LANG = 'Japanese' #@param [\"Japanese\", \"English\", \"Chinese\"]\n",
        "# English\n",
        "# Chinese\n",
        "# Japanese\n",
        "\n",
        "# モデルのコンテキストの長さ\n",
        "ctx_len = 1024 #@param {type:\"integer\"}\n",
        "\n",
        "# chat.pyの内容を置換するための辞書を作成\n",
        "replacements = {\n",
        "    'args.strategy = .*' : 'args.strategy = \\'' + strategy + '\\'',\n",
        "    'os\\.environ\\[\\\"RWKV_CUDA_ON\\\"\\] = \\'.' : 'os.environ[\"RWKV_CUDA_ON\"] = \\'' + str(RWKV_CUDA_ON),\n",
        "    'CHAT_LANG = .*' : 'CHAT_LANG = \\'' + CHAT_LANG + '\\'',\n",
        "    'args.MODEL_NAME = .*' : 'args.MODEL_NAME = \\'' + MODEL_NAME + '\\'',\n",
        "    'args.ctx_len = .*' : 'args.ctx_len = \\'' + str(ctx_len) + '\\'',\n",
        "    'current_path = os\\.path\\.dirname\\(os\\.path\\.abspath\\(__file__\\)\\)' : 'current_path = os.getcwd()',\n",
        "    'while True:\\s+msg = prompt.+\\s+if len\\(msg.+\\s+on_message\\(msg\\)\\s+else:\\s+print\\(.+' : ''\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkBn_XeEJinG"
      },
      "outputs": [],
      "source": [
        "# chat.pyの変数を置換してchat-notebook.pyに保存\n",
        "import re\n",
        "\n",
        "with open('chat.py', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "for old, new in replacements.items():\n",
        "    pattern = re.compile(r'^(\\s*)' + old, flags=re.MULTILINE)\n",
        "    text = pattern.sub(r'\\1' + new, text)\n",
        "\n",
        "with open('chat-notebook.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1PB9PyA28k-",
        "outputId": "9d8b9941-25a2-4a2b-f733-4ef8b55cadc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ChatRWKV v2 https://github.com/BlinkDL/ChatRWKV\n",
            "\n",
            "Japanese - cuda fp16 - /content/ChatRWKV/v2/prompt/default/Japanese-2.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py39_cu118/wkv_cuda...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py39_cu118/wkv_cuda/build.ninja...\n",
            "Building extension module wkv_cuda...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module wkv_cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model - RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096\n",
            "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
            "\n",
            "Loading RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096.pth ...\n",
            "Strategy: (total 32+1=33 layers)\n",
            "* cuda [float16, float16], store 33 layers\n",
            "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
            "emb.weight                        f16      cpu  50277  2560 \n",
            "blocks.0.ln1.weight               f16   cuda:0   2560       \n",
            "blocks.0.ln1.bias                 f16   cuda:0   2560       \n",
            "blocks.0.ln2.weight               f16   cuda:0   2560       \n",
            "blocks.0.ln2.bias                 f16   cuda:0   2560       \n",
            "blocks.0.att.time_decay           f32   cuda:0   2560       \n",
            "blocks.0.att.time_first           f32   cuda:0   2560       \n",
            "blocks.0.att.time_mix_k           f16   cuda:0   2560       \n",
            "blocks.0.att.time_mix_v           f16   cuda:0   2560       \n",
            "blocks.0.att.time_mix_r           f16   cuda:0   2560       \n",
            "blocks.0.att.key.weight           f16   cuda:0   2560  2560 \n",
            "blocks.0.att.value.weight         f16   cuda:0   2560  2560 \n",
            "blocks.0.att.receptance.weight    f16   cuda:0   2560  2560 \n",
            "blocks.0.att.output.weight        f16   cuda:0   2560  2560 \n",
            "blocks.0.ffn.time_mix_k           f16   cuda:0   2560       \n",
            "blocks.0.ffn.time_mix_r           f16   cuda:0   2560       \n",
            "blocks.0.ffn.key.weight           f16   cuda:0   2560 10240 \n",
            "blocks.0.ffn.receptance.weight    f16   cuda:0   2560  2560 \n",
            "blocks.0.ffn.value.weight         f16   cuda:0  10240  2560 \n",
            "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "blocks.31.ln1.weight              f16   cuda:0   2560       \n",
            "blocks.31.ln1.bias                f16   cuda:0   2560       \n",
            "blocks.31.ln2.weight              f16   cuda:0   2560       \n",
            "blocks.31.ln2.bias                f16   cuda:0   2560       \n",
            "blocks.31.att.time_decay          f32   cuda:0   2560       \n",
            "blocks.31.att.time_first          f32   cuda:0   2560       \n",
            "blocks.31.att.time_mix_k          f16   cuda:0   2560       \n",
            "blocks.31.att.time_mix_v          f16   cuda:0   2560       \n",
            "blocks.31.att.time_mix_r          f16   cuda:0   2560       \n",
            "blocks.31.att.key.weight          f16   cuda:0   2560  2560 \n",
            "blocks.31.att.value.weight        f16   cuda:0   2560  2560 \n",
            "blocks.31.att.receptance.weight   f16   cuda:0   2560  2560 \n",
            "blocks.31.att.output.weight       f16   cuda:0   2560  2560 \n",
            "blocks.31.ffn.time_mix_k          f16   cuda:0   2560       \n",
            "blocks.31.ffn.time_mix_r          f16   cuda:0   2560       \n",
            "blocks.31.ffn.key.weight          f16   cuda:0   2560 10240 \n",
            "blocks.31.ffn.receptance.weight   f16   cuda:0   2560  2560 \n",
            "blocks.31.ffn.value.weight        f16   cuda:0  10240  2560 \n",
            "ln_out.weight                     f16   cuda:0   2560       \n",
            "ln_out.bias                       f16   cuda:0   2560       \n",
            "head.weight                       f16   cuda:0   2560 50277 \n",
            "\n",
            "Run prompt...\n",
            "コマンド:\n",
            "直接入力 --> ボットとチャットする．改行には\\nを使用してください．\n",
            "+ --> ボットに前回のチャットの内容を変更させる．\n",
            "+reset --> 対話のリセット．メモリをリセットするために，+resetを定期的に実行してください．\n",
            "\n",
            "+i インストラクトの入力 --> チャットの文脈を無視して独立した質問を行う．改行には\\nを使用してください．\n",
            "+gen プロンプトの生成 --> チャットの文脈を無視して入力したプロンプトに続く文章を出力する．改行には\\nを使用してください．\n",
            "+++ --> +gen / +i の出力の回答を続ける．\n",
            "++ --> +gen / +i の出力の再生成を行う.\n",
            "\n",
            "ボットとの会話を楽しんでください。また、定期的に+resetして、ボットのメモリをリセットすることを忘れないようにしてください。\n",
            "\n",
            "Japanese - RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096 - cuda fp16\n",
            "\n",
            "以下は、Aliceという女の子とその友人Bobの間で行われた会話です。 Aliceはとても賢く、想像力があり、友好的です。 AliceはBobに反対することはなく、AliceはBobに質問するのは苦手です。 AliceはBobに自分のことや自分の意見をたくさん伝えるのが好きです。 AliceはいつもBobに親切で役に立つ、有益なアドバイスをしてくれます。\n",
            "\n",
            "Bob: こんにちはAlice、調子はどうですか？\n",
            "Alice: こんにちは！元気ですよ。あたなはどうですか？\n",
            "\n",
            "Bob: 元気ですよ。君に会えて嬉しいよ。見て、この店ではお茶とジュースが売っているよ。\n",
            "Alice: 本当ですね。中に入りましょう。大好きなモカラテを飲んでみたいです！\n",
            "\n",
            "Bob: モカラテって何ですか？\n",
            "Alice: モカラテはエスプレッソ、ミルク、チョコレート、泡立てたミルクから作られた飲み物です。香りはとても甘いです。\n",
            "\n",
            "Bob: それは美味しそうですね。今度飲んでみます。しばらく私とおしゃべりしてくれますか？\n",
            "Alice: もちろん！ご質問やアドバイスがあれば、喜んでお答えします。専門的な知識には自信がありますよ。どうぞよろしくお願いいたします！\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# モデルをロード\n",
        "execfile(\"chat-notebook.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# チャット"
      ],
      "metadata": {
        "id": "SW7tRuFSGxj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCcLpH_GpXEI"
      },
      "outputs": [],
      "source": [
        "# 文章生成に関する設定\n",
        "\n",
        "# チャットの短い応答の長さ\n",
        "CHAT_LEN_SHORT = 40 #@param {type:\"integer\"}\n",
        "# チャットの長い応答の長さ\n",
        "CHAT_LEN_LONG = 150 #@param {type:\"integer\"}\n",
        "# 自由生成文の長さ\n",
        "FREE_GEN_LEN = 256 #@param {type:\"integer\"}\n",
        "\n",
        "# For better chat & QA quality: reduce temp, reduce top-p, increase repetition penalties\n",
        "# Explanation: https://platform.openai.com/docs/api-reference/parameter-details\n",
        "\n",
        "# GEN_TEMPとGEN_TOP_P: 小さい値では正確さが上がり、大きい値では多様性が上がる\n",
        "GEN_TEMP = 1.1 #@param {type:\"number\"} # sometimes it's a good idea to increase temp. try it\n",
        "GEN_TOP_P = 0.7 #@param {type:\"number\"}\n",
        "# GEN_alpha_presence, GEN_alpha_frequency: 繰り返し文字列の存在と頻度に対するペナルティ。大きい値では繰り返しが抑制される。\n",
        "GEN_alpha_presence = 0.2 #@param {type:\"number\"} # Presence Penalty\n",
        "GEN_alpha_frequency = 0.2 #@param {type:\"number\"} # Frequency Penalty\n",
        "# AVOID_REPEAT: 繰り返しを防ぐ文字\n",
        "AVOID_REPEAT = '，：？！' #@param {type:\"string\"}\n",
        "# 入力を分割するチャンク長\n",
        "CHUNK_LEN = 256 #@param {type:\"integer\"} # split input into chunks to save VRAM (shorter -> slower)\n",
        "\n",
        "AVOID_REPEAT_TOKENS = []\n",
        "for i in AVOID_REPEAT:\n",
        "    dd = pipeline.encode(i)\n",
        "    assert len(dd) == 1\n",
        "    AVOID_REPEAT_TOKENS += dd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFnPmwyp5fCA",
        "outputId": "c8aa3ee5-5711-426c-d589-c37244024e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bob: こんにちは\n",
            "Alice: こんにちは！何かお茶を注文してくれますか？\n",
            "\n",
            "Bob: 日本の首都はどこですか？\n",
            "Alice: 日本の首都は東京です。今日は湯川若菜さんにお話を伺いました。日本の首都の中でも、東京は世界で最も有名な都市です。東京には著名な商店街や国立公園があります。\n",
            "\n",
            "Bob: 旅行は好きですか？\n",
            "Alice: はい、日本の旅行は素晴らしいですね！都市を探検し、山や海、峡谷を歩くことで、大自然を体験することができます。東京は地域によって、国内外から人気があります。さらに、湯川若菜さんにアドバイスしてみたいと言われるときは、特に何もしなくても良いですね。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# チャットの実行\n",
        "while True:\n",
        "    msg = input(f'{user}{interface} ')\n",
        "    if len(msg.strip()) > 0:\n",
        "        on_message(msg)\n",
        "    else:\n",
        "        print('Error: please say something')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPPHIno/7SqYji12s6AV5kP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}